
# Initial Data Import -----------------------------------------------------

library(data.table) # install data-table for data manipulation
library(lubridate) # package for date manipulation (for time of day)

# loaded in data into AWS in 3 chunks (split using linux)
# get training, initial validation, secondary validation data by combining uploaded file chunks
train <- rbind(fread("Train-000.csv"), 
               fread("Train-001.csv"), 
               fread("Train-002.csv"), 
               fread("Train-003.csv"))

val1 <- rbind(fread("Val1-000.csv"), fread("Val1-001.csv"))
val2 <- rbind(fread("Val2-000.csv"), fread("Val2-001.csv"))
test <- rbind(fread("Test-000.csv"), fread("Test-001.csv"), fread("Test-002.csv"))

# get rid of row id columns generated by Excel
train <- train[, 2:25]
val1 <- val1[, 2:25]
val2 <- val2[, 2:25]

# Prep Data -----------------------------------------------------------

# import data and set up column names
data <- copy(train) # load copy of train data so original is not effected
names <- c("id", "click", "time", "C1", "banner_pos", "site_id", "site_domain", "site_category", "app_id", 
           "app_domain", "app_category", "device_id", "device_ip", "device_model", "device_type", "device_conn_type", 
           "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21")
colnames(data) <- names # assign column names to data
head(data) # inspect first 5 rows

# get relevant time-based variables e.g. hour of day and day of week
data[, time:= as.POSIXct(as.character(data$time), format="%y%m%d%H")] # convert format of time variable to date
data[, hour:= hour(time) ] # get hour of day from date
data[, day_of_week := weekdays(time)] # get day of week from date
data$time <- NULL # get rid of broader time variable since no longer needed

# based on earlier exploration and business consideration, choose to drop device id and device ip
# these variables are presumably unavailable at the time a prediction is being made
data$device_id <- NULL # Over 80% of values belong to one level, likely an unknown
data$device_ip <- NULL # High fragmentation, only one value takes on more than 0.5% of data

# make all categorical variables strings (relevant for levels re-assignment from next section)
factor_cols <- colnames(data[,3:23]) # get names of categorical variables
data[, (factor_cols) := lapply(.SD, as.character), .SDcols=3:23]

# Categorical Data Prep ---------------------------------------------------

# create a copy of original data to enable categorical level manipulation
factors_data <- copy(data)

# assign categorical variables as factors
factors_data[, (factor_cols) := lapply(.SD, as.factor), .SDcols=3:23] # asisgn all categorical variables as factors for relevant columns

# get number of levels for each categorical variable
num_levels_dt <- factors_data[, lapply(.SD, function(x) length(levels(x))), .SDcols=3:23] # get table with frequency of occurrence for each variable
num_levels_dt <- melt(num_levels_dt,  measure.vars=c(1:21), variable.name="variable", value.name="frequency") # transpose format to get a frequency table

# get subset list of variables that have too many levels and need to be 'downsized' using pareto principle
# work on variables with more than 30 levels since most algorithms take a max of 32 levels
fix_levels_dt <- num_levels_dt[frequency > 30,]

# for each variable to fix, get data table with count and proportion of values by each unique level, ordered from highest to lowest 
num_rows <- nrow(factors_data) # get total number of rows in data for proportion calculation
freq_tables_list <- data.table() # initialize an empty list

# store a datatable that has the frequency count for each variable to fix in a nested list
for (n in fix_levels_dt$variable){ # loop through variable names that need fewer levels
  temp <- factors_data[, .(freq=.N, prop=.N/num_rows), by=n][order(-freq)] # get data table containing each variable's data by level
  freq_tables_list <- c(freq_tables_list, list(temp))
}
names(freq_tables_list) <- fix_levels_dt$variable # ensure list names map to variables

# for loop that creates a lookup table that maps original --> new levels for each categorical variable based on training data
for (var in fix_levels_dt$variable) {
  # get data table for current variable
  dt <- freq_tables_list[[var]]
  
  # get data table that will serve as lookup key for current variable levels
  name <- paste(var, "_lookup", sep="") 
  assign(name, data.table("original"=unique(data[[var]])))
  get(name)[, new:= original]
  
  # get levels to treat as other (those with less than 0.05% of total observations each)
  other_labels <- as.character(dt[prop<0.0005][[var]])
  
  # get levels to run regression on
  regress_labels <- as.character(dt[prop<0.01 & prop>=0.0005][[var]]) # get levels with prop of rows between 1% and 0.05%
  data_regress <- factors_data[get(var) %in% regress_labels] # get data subset with levels to regress
  
  # run logistic regression on levels to regress
  glm_levels_select <- glm(click ~ get(var), family=binomial(link="logit"), data=data_regress) # run logistic regression
  
  # get data table with variable names, coefficients, and p-values
  data_factors_groups <- data.table("variable"=variable.names(glm_levels_select), 
                                    "coef"=glm_levels_select$coefficients, 
                                    "p_value"=summary(glm_levels_select)$coefficients[,4])
  
  data_factors_groups <- data_factors_groups[variable!="(Intercept)"] # remove intercepts from table
  data_factors_groups[, variable:= gsub(pattern="get\\(var\\)", replacement="", data_factors_groups$variable)] # clean variable names
  
  # add non-significant variables to 'other' category
  other_labels <- c(other_labels, data_factors_groups[p_value > 0.1, variable])
  
  # get rid of non-significant coefficients for creating additional variable categories
  data_factors_groups <- data_factors_groups[p_value<0.1, ] 
  data_factors_groups$p_value <- NULL # remove p-value column since it is not needed
  
  # get 8 additional levels of categorical variables, based on significant coefficient quartile for positive and negative coefficient types
  data_factors_groups[, value_type := ifelse(coef>=0, "Positive", "Negative")]
  
  # conditional that updates the level label based on quartile number (1-4) and direction (pos/neg) of its coefficient
  if (nrow(data_factors_groups[value_type=="Positive",])>0) {
    quartiles_pos <- data_factors_groups[value_type=="Positive",quantile(coef)] # get positive quartiles
    data_factors_groups[value_type=="Positive", quartile := ifelse(coef < quartiles_pos[[2]], "Q1_Pos",
                                                                   ifelse(coef < quartiles_pos[[3]], "Q2_Pos",
                                                                          ifelse(coef < quartiles_pos[[4]], "Q3_Pos", "Q4_Pos")))] # assign positive quartiles
  }
  
  if (nrow(data_factors_groups[value_type=="Negative",])>0) {
    quartiles_neg <- data_factors_groups[value_type=="Negative",quantile(coef)] # get negative qartiles
    data_factors_groups[value_type=="Negative", quartile := ifelse(coef < quartiles_neg[[2]], "Q1_Neg",
                                                                   ifelse(coef < quartiles_neg[[3]], "Q2_Neg",
                                                                          ifelse(coef < quartiles_neg[[4]], "Q3_Neg", "Q4_Neg")))] # assign negative quartiles
  }
  
  
  # create list that contains value names of each coefficient quartile/value type
  q1_pos_labels <- data_factors_groups[quartile=="Q1_Pos", variable]
  q2_pos_labels <- data_factors_groups[quartile=="Q2_Pos", variable]
  q3_pos_labels <- data_factors_groups[quartile=="Q3_Pos", variable]
  q4_pos_labels <- data_factors_groups[quartile=="Q4_Pos", variable]
  q1_neg_labels <- data_factors_groups[quartile=="Q1_Neg", variable]
  q2_neg_labels <- data_factors_groups[quartile=="Q2_Neg", variable]
  q3_neg_labels <- data_factors_groups[quartile=="Q3_Neg", variable]
  q4_neg_labels <- data_factors_groups[quartile=="Q4_Neg", variable]
  
  # update all values based on new levels
  get(name)[original %in% other_labels, new:="Other"]
  get(name)[original %in% q1_pos_labels, new:= "Q1_Pos"]
  get(name)[original %in% q2_pos_labels, new:= "Q2_Pos"]
  get(name)[original %in% q3_pos_labels, new:= "Q3_Pos"]
  get(name)[original %in% q4_pos_labels, new:= "Q4_Pos"]
  get(name)[original %in% q1_neg_labels, new:= "Q1_Neg"]
  get(name)[original %in% q2_neg_labels, new:= "Q2_Neg"]
  get(name)[original %in% q3_neg_labels, new:= "Q3_Neg"]
  get(name)[original %in% q4_neg_labels, new:= "Q4_Neg"]
}

# use lookup key by column to update variable values in original data, rely on function
update_categories <- function(data) {
  dt <- copy(data)
  for (var in fix_levels_dt$variable) {
    # get name of data table that has lookup information
    name_lookup <- paste(var, "_lookup", sep="") 
    # merge data with current file and assign value from 'new' level label
    dt <- merge(dt, get(name_lookup), all.x=TRUE, by.x=var, by.y="original")
    dt[is.na(new), new := "Other"] # if level hasn't been seen before make it 'other'
    dt[, (var):= new]
    dt[, new:= NULL]
  }
  return(dt)
}

train_cleaned <- update_categories(data) # update categories for training data
train_cleaned[, (factor_cols) := lapply(.SD, as.factor), .SDcols=(factor_cols)] # set categories as factors
train_cleaned[, lapply(.SD, function(x) length(levels(x)))] # inspect number of levels by variables


# Apply cleaning to validation and test datasets ---------------------------------------------------

# function that does data prep
data_prep <- function(data) {
  # update columnnames
  dt <- copy(data)
  names <- c("id", "click", "time", "C1", "banner_pos", "site_id", "site_domain", "site_category", "app_id", "app_domain", "app_category", "device_id", "device_ip", "device_model", "device_type", "device_conn_type", "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21")
  colnames(dt) <- names # assign column names to data
  
  # get relevant time-based variables e.g. hour of day and day of week
  dt[, time:= as.POSIXct(as.character(dt$time), format="%y%m%d%H")] # convert format of time variable to date
  dt[, hour:= hour(time) ] # get hour of day from date
  dt[, day_of_week := weekdays(time)] # get day of week from date
  dt$time <- NULL # get rid of broader time variable since no longer needed
  
  # based on earlier exploration and business consideration, choose to drop device id and device ip
  dt$device_id <- NULL
  dt$device_ip <- NULL
  
  # make all categorical variables strings (relevant for levels re-assignment from next section)
  factor_cols <- colnames(dt[,3:23]) # get names of categorical variables
  dt[, (factor_cols) := lapply(.SD, as.character), .SDcols=3:23]
  
  # update levels of categorical variables and store
  dt <- update_categories(dt) # update categories for training data
  dt[, (factor_cols) := lapply(.SD, as.factor), .SDcols=(factor_cols)] # set categories as factors
  
  return(dt)
}

# apply function that does data prep on each validation data file
## Note: these were later merged into a single validation data file, 
## but the prep code took a substantial amount of time so this was left as-is
val1_cleaned <- data_prep(val1)
val2_cleaned <- data_prep(val2)

# adjusted function to prep test data (given that click variable is not included)
data_prep_test <- function(data) {
  dt <- copy(test)
  names <- c("id", "time", "C1", "banner_pos", "site_id", "site_domain", "site_category", "app_id", "app_domain", "app_category", "device_id", "device_ip", "device_model", "device_type", "device_conn_type", "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21")
  colnames(dt) <- names # assign column names to data
  
  # get relevant time-based variables e.g. hour of day and day of week
  dt[, time:= as.POSIXct(as.character(dt$time), format="%y%m%d%H")] # convert format of time variable to date
  dt[, hour:= hour(time) ] # get hour of day from date
  dt[, day_of_week := weekdays(time)] # get day of week from date
  dt$time <- NULL # get rid of broader time variable since no longer needed
  
  # based on earlier exploration and business consideration, choose to drop device id and device ip
  dt$device_id <- NULL
  dt$device_ip <- NULL
  
  # make all categorical variables strings (relevant for levels re-assignment from next section)
  factor_cols <- colnames(dt[,2:22]) # get names of categorical variables
  dt[, (factor_cols) := lapply(.SD, as.character), .SDcols=2:22]
  
  # update levels of categorical variables and store
  dt <- update_categories(dt) # update categories for training data
  dt[, (factor_cols) := lapply(.SD, as.factor), .SDcols=(factor_cols)] # set categories as factors
  
  return(dt)
}

test_cleaned <- data_prep_test(test) # clean test data

# reorder column names in cleaned data to enable smoother manipulation
# store desired column order for training and validation data
main_col_order <- c("id", "click", "C1", "banner_pos", "site_id", "site_domain", "site_category", "app_id",
                    "app_domain", "app_category", "device_model", "device_type", "device_conn_type",
                    "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21", "hour", "day_of_week")

# assign desired column order to training and validation data
setcolorder(train_cleaned, main_col_order)
setcolorder(val1_cleaned, main_col_order)
setcolorder(val2_cleaned, main_col_order)

# store desired order for test data (which excludes click column)
test_col_order <- c("id", "C1", "banner_pos", "site_id", "site_domain", "site_category", "app_id",
                    "app_domain", "app_category", "device_model", "device_type", "device_conn_type",
                    "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21", "hour", "day_of_week")

# assign desired column order to test data
setcolorder(test_cleaned, test_col_order)


# Save Files For Export ---------------------------------------------------

save(train_cleaned, file="train_clean.RData")
save(val1_cleaned, file="val1_clean.RData")
save(val2_cleaned, file="val2_clean.RData")
save(test_cleaned, file="test_clean.RData")
